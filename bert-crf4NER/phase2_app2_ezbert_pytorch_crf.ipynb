{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "APR_DIR = \"phase2_apr2/\"\n",
    "DATA_DIR =\"new_tags_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def seed_torch(seed=12345):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-location': 0, 'O': 1, 'I-location': 2, 'B-date': 3, 'B-person_name': 4, 'I-person_name': 5, 'B-id': 6, 'B-age': 7, 'B-hospital_name': 8, 'I-hospital_name': 9, 'B-room_no': 10, 'I-id': 11, 'I-age': 12, 'I-date': 13, 'B-telephone_no': 14, 'I-telephone_no': 15, 'B-org_name': 16, 'I-org_name': 17, 'I-room_no': 18, 'B-web_url': 19, 'I-web_url': 20, 'X': 21}\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "LIM =None\n",
    "def corpus_reader(path, delim='\\t', word_idx=0, fname_idx=1, label_idx=-1):\n",
    "  tokens, file_names, labels = [], [], []\n",
    "  tmp_tok, tmp_fname, tmp_lab = [], [], []\n",
    "  label_set = []\n",
    "  with open(path, 'r') as reader:\n",
    "    for line in reader:\n",
    "      line = line.strip()\n",
    "      cols = line.split(delim)\n",
    "      #print(len(cols))\n",
    "      if len(cols) < 2:\n",
    "        if len(tmp_tok) > 0:\n",
    "            tokens.append(tmp_tok); file_names.append(tmp_fname); labels.append(tmp_lab)\n",
    "        tmp_tok = [] \n",
    "        tmp_fname = []\n",
    "        tmp_lab = []\n",
    "        continue\n",
    "      else:\n",
    "        tmp_tok.append(cols[word_idx])\n",
    "        tmp_fname.append(cols[fname_idx])\n",
    "        tmp_lab.append(cols[label_idx])\n",
    "        label_set.append(cols[label_idx])\n",
    "  return tokens[:LIM], file_names[:LIM], labels[:LIM], list(OrderedDict.fromkeys(label_set))\n",
    "\n",
    "_, fnames, _, label_set = corpus_reader(DATA_DIR +'train_SL200.txt', delim='\\t')\n",
    "label_set.append('X')\n",
    "#tag2idx = {'B-location': 0, 'O': 1, 'I-location': 2, 'X': 3}\n",
    "tag2idx = {t: i for i, t in enumerate(label_set)}\n",
    "#tag2idx = {'B-location': 0, 'O': 1, 'I-location': 2, 'X': 3}\n",
    "\n",
    "print(tag2idx)\n",
    "unique_labels = list(label_set)\n",
    "print(len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(APR_DIR + 'tag2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(tag2idx, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(APR_DIR + 'unique_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_labels, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'B-location': 0, 'O': 1, 'I-location': 2, 'B-date': 3, 'B-person_name': 4, 'I-person_name': 5, 'B-id': 6, 'B-age': 7, 'B-hospital_name': 8, 'I-hospital_name': 9, 'B-room_no': 10, 'I-id': 11, 'I-age': 12, 'I-date': 13, 'B-telephone_no': 14, 'I-telephone_no': 15, 'B-org_name': 16, 'I-org_name': 17, 'I-room_no': 18, 'B-web_url': 19, 'I-web_url': 20, 'X': 21}\n",
    "# # 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['287305_2781269_20180805150026.624',\n",
       " '287305_2781269_20180805150026.624',\n",
       " '287305_2781269_20180805150026.624',\n",
       " '287305_2781269_20180805150026.624',\n",
       " '287305_2781269_20180805150026.624']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 10:59:03.279425 140235405039360 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1212 10:59:03.608171 140235405039360 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data \n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class NER_Dataset(data.Dataset):\n",
    "    def __init__(self, sentences, fnames, labels, tokenizer_path = '', use_bert_emb=True):\n",
    "       self.sentences = sentences\n",
    "       self.fnames = fnames\n",
    "       self.labels = labels\n",
    "       self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = []\n",
    "        for x in self.labels[idx]:\n",
    "            if x in tag2idx.keys():\n",
    "                label.append(tag2idx[x])\n",
    "            else:\n",
    "                label.append(tag2idx['O'])\n",
    "        bert_tokens = []\n",
    "        orig_to_tok_map = []\n",
    "        bert_tokens.append('[CLS]')\n",
    "        modified_labels = [tag2idx['X']]\n",
    "        for i, token in enumerate(sentence):\n",
    "          if len(bert_tokens) >= 512:\n",
    "            break\n",
    "          orig_to_tok_map.append(len(bert_tokens))\n",
    "          modified_labels.append(label[i])\n",
    "          new_token = self.tokenizer.tokenize(token)\n",
    "          bert_tokens.extend(new_token)\n",
    "          modified_labels.extend([tag2idx['X']] * (len(new_token) -1))\n",
    "          \n",
    "        bert_tokens.append('[SEP]')\n",
    "        modified_labels.append(tag2idx['X'])\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "        f_name = self.fnames[idx][0]\n",
    "        if len(token_ids) > 511:\n",
    "            token_ids = token_ids[:512]\n",
    "            modified_labels = modified_labels[:512]\n",
    "            #f_name = f_name[:512]\n",
    "        # tokens_ids, lenth of tokens(use for dynamic padding), orig_to_tok_map(will used for prediction) labels, original_token\n",
    "        f_names = [f_name] * len(token_ids)\n",
    "        return token_ids, len(token_ids), orig_to_tok_map, modified_labels, sentence, f_names\n",
    " \n",
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    get_element = lambda x: [sample[x] for sample in batch]\n",
    "    seq_len = get_element(1)\n",
    "    maxlen = np.array(seq_len).max()\n",
    "    do_pad = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    tok_ids = do_pad(0, maxlen)\n",
    "    attn_mask = [[(i>0) for i in ids] for ids in tok_ids] \n",
    "    LT = torch.LongTensor\n",
    "    label = do_pad(-3, maxlen)\n",
    "    \n",
    "    token_ids = get_element(0)\n",
    "    token_ids_len = torch.LongTensor(list(map(len, token_ids)))\n",
    "    _, sorted_idx = token_ids_len.sort(0, descending=True)\n",
    "    \n",
    "    #sorting based on padding size\n",
    "    tok_ids = LT(tok_ids)[sorted_idx]\n",
    "    attn_mask = LT(attn_mask)[sorted_idx]\n",
    "    label = LT(label)[sorted_idx]\n",
    "    \n",
    "    #orig_to_tok_map = LT(get_element(2))[sorted_idx]\n",
    "    \n",
    "    unsorted_orig_tok_map = get_element(2)\n",
    "    unsorted_sent = get_element(-2)\n",
    "    unsorted_f_names = get_element(-1)\n",
    "    \n",
    "    sorted_sent = []\n",
    "    sorted_f_names = []\n",
    "    sorted_org_tok_map = []\n",
    "    for i in sorted_idx.cpu().numpy():\n",
    "        sorted_sent.append(unsorted_sent[i])\n",
    "        sorted_f_names.append(unsorted_f_names[i])\n",
    "        sorted_org_tok_map.append(unsorted_orig_tok_map[i])\n",
    "    return tok_ids, attn_mask, sorted_org_tok_map, label, sorted_sent, sorted_f_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from torchcrf import CRF\n",
    "#from torch.nn import log_softmax\n",
    "log_soft = F.log_softmax\n",
    "\n",
    "class Bert_CRF(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(Bert_CRF, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        print(self.num_labels)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "        self.crf = CRF(self.num_labels, batch_first=True)    \n",
    "    \n",
    "    def forward(self, input_ids, attn_masks, labels=None):  # dont confuse this with _forward_alg above.\n",
    "        outputs = self.bert(input_ids, attn_masks)\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emission = self.classifier(sequence_output)\n",
    "        \n",
    "        #emission = self.classifier(lstm_out)\n",
    "        attn_masks = attn_masks.type(torch.uint8)\n",
    "        #labels = labels.type(torch.uint8)\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(log_soft(emission, 2), labels, mask=attn_masks, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            prediction = self.crf.decode(emission, mask=attn_masks)\n",
    "            return prediction\n",
    "        #return scores, tag_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 10:59:09.595587 140235405039360 tokenization_utils.py:306] Model name '/home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps' is a path or url to a directory containing tokenizer files.\n",
      "I1212 10:59:09.596513 140235405039360 tokenization_utils.py:335] Didn't find file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/added_tokens.json. We won't load it.\n",
      "I1212 10:59:09.597045 140235405039360 tokenization_utils.py:335] Didn't find file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/special_tokens_map.json. We won't load it.\n",
      "I1212 10:59:09.597518 140235405039360 tokenization_utils.py:335] Didn't find file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/tokenizer_config.json. We won't load it.\n",
      "I1212 10:59:09.598042 140235405039360 tokenization_utils.py:371] loading file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/vocab.txt\n",
      "I1212 10:59:09.598503 140235405039360 tokenization_utils.py:371] loading file None\n",
      "I1212 10:59:09.598960 140235405039360 tokenization_utils.py:371] loading file None\n",
      "I1212 10:59:09.599390 140235405039360 tokenization_utils.py:371] loading file None\n",
      "I1212 10:59:11.672404 140235405039360 tokenization_utils.py:306] Model name '/home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps' is a path or url to a directory containing tokenizer files.\n",
      "I1212 10:59:11.673431 140235405039360 tokenization_utils.py:335] Didn't find file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/added_tokens.json. We won't load it.\n",
      "I1212 10:59:11.673915 140235405039360 tokenization_utils.py:335] Didn't find file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/special_tokens_map.json. We won't load it.\n",
      "I1212 10:59:11.674380 140235405039360 tokenization_utils.py:335] Didn't find file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/tokenizer_config.json. We won't load it.\n",
      "I1212 10:59:11.674896 140235405039360 tokenization_utils.py:371] loading file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/vocab.txt\n",
      "I1212 10:59:11.675335 140235405039360 tokenization_utils.py:371] loading file None\n",
      "I1212 10:59:11.675791 140235405039360 tokenization_utils.py:371] loading file None\n",
      "I1212 10:59:11.676280 140235405039360 tokenization_utils.py:371] loading file None\n"
     ]
    }
   ],
   "source": [
    "#tokens, file_names, labels, label_set\n",
    "train_sentences, file_names, train_labels, _ = corpus_reader(DATA_DIR + 'train_SL200.txt', delim='\\t')\n",
    "train_dataset = NER_Dataset(train_sentences, file_names, train_labels, tokenizer_path = '/home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps')\n",
    "#eval_dataset = STSDataset(id_2_example, test_ids, Param)\n",
    "\n",
    "dev_sentences, dev_file_names, dev_labels, _ = corpus_reader(DATA_DIR + 'test.txt', delim='\\t')\n",
    "dev_dataset = NER_Dataset(dev_sentences, dev_file_names, dev_labels, tokenizer_path = '/home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386750"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)\n",
    "#386750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=16,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=pad)\n",
    "eval_iter = data.DataLoader(dataset=dev_dataset,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,\n",
    "                            num_workers=1,\n",
    "                            collate_fn=pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 10:59:11.747467 140235405039360 configuration_utils.py:148] loading configuration file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/config.json\n",
      "I1212 10:59:11.748398 140235405039360 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 22,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1212 10:59:11.749823 140235405039360 modeling_utils.py:334] loading weights file /home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Tags:  22\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1212 10:59:15.532253 140235405039360 modeling_utils.py:405] Weights of Bert_CRF not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
      "I1212 10:59:15.533070 140235405039360 modeling_utils.py:408] Weights from pretrained model not used in Bert_CRF: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import subprocess\n",
    "from transformers import BertModel \n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "print('#Tags: ', len(tag2idx))\n",
    "model = Bert_CRF.from_pretrained(\"/home/ubuntu/dhana/EzBert/bert/our_pretrained_uncase/model_with_SL_128_9KSteps\", num_labels = len(tag2idx))\n",
    "#print(model.parameters())\n",
    "if torch.cuda.is_available():\n",
    "  model.cuda()\n",
    "\n",
    "num_epoch = 16\n",
    "gradient_acc_steps = 1\n",
    "t_total = len(train_iter) // gradient_acc_steps * num_epoch\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "global_step = 0\n",
    "#tr_loss, logging_loss = 0.0\n",
    "model.zero_grad()\n",
    "model.train()\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "pearson_score = []\n",
    "train_iterator = trange(num_epoch, desc=\"Epoch\", disable=0)\n",
    "best_f1 = 0.0\n",
    "tmp_loss = 0.0\n",
    "MAX_SCORE = 0.0 \n",
    "MAX_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 10:59:20.091209 Step: 1 of 24172 Loss: 34.825382\n",
      "2019-12-12 11:01:54.935637 Step: 1000 of 24172 Loss: 1.188062\n",
      "2019-12-12 11:04:32.597347 Step: 2000 of 24172 Loss: 0.479009\n",
      "2019-12-12 11:07:07.274856 Step: 3000 of 24172 Loss: 0.447093\n",
      "2019-12-12 11:09:41.929404 Step: 4000 of 24172 Loss: 0.354401\n",
      "2019-12-12 11:12:17.706725 Step: 5000 of 24172 Loss: 0.397724\n",
      "2019-12-12 11:14:56.801268 Step: 6000 of 24172 Loss: 0.431707\n",
      "2019-12-12 11:17:28.521043 Step: 7000 of 24172 Loss: 0.284069\n",
      "2019-12-12 11:20:05.555144 Step: 8000 of 24172 Loss: 0.345935\n",
      "2019-12-12 11:22:40.061125 Step: 9000 of 24172 Loss: 0.263037\n",
      "2019-12-12 11:25:14.697096 Step: 10000 of 24172 Loss: 0.307490\n",
      "2019-12-12 11:27:50.023089 Step: 11000 of 24172 Loss: 0.285687\n",
      "2019-12-12 11:30:22.351705 Step: 12000 of 24172 Loss: 0.259938\n",
      "2019-12-12 11:32:56.236215 Step: 13000 of 24172 Loss: 0.238009\n",
      "2019-12-12 11:35:29.513624 Step: 14000 of 24172 Loss: 0.268181\n",
      "2019-12-12 11:38:08.653153 Step: 15000 of 24172 Loss: 0.240225\n",
      "2019-12-12 11:40:49.048628 Step: 16000 of 24172 Loss: 0.260328\n",
      "2019-12-12 11:43:24.674322 Step: 17000 of 24172 Loss: 0.209685\n",
      "2019-12-12 11:45:56.920423 Step: 18000 of 24172 Loss: 0.232591\n",
      "2019-12-12 11:48:32.810391 Step: 19000 of 24172 Loss: 0.248879\n",
      "2019-12-12 11:51:07.566626 Step: 20000 of 24172 Loss: 0.201264\n",
      "2019-12-12 11:53:43.215381 Step: 21000 of 24172 Loss: 0.230691\n",
      "2019-12-12 11:56:14.809009 Step: 22000 of 24172 Loss: 0.219687\n",
      "2019-12-12 11:58:48.627296 Step: 23000 of 24172 Loss: 0.268109\n",
      "2019-12-12 12:01:21.369855 Step: 24000 of 24172 Loss: 0.207842\n",
      "Training Loss: 0.326914 for epoch 0\n",
      "Epoch:  0\n",
      "processed 917637 tokens with 33613 phrases; found: 34575 phrases; correct: 32512.\n",
      "accuracy:  97.34%; (non-O)\n",
      "accuracy:  99.65%; precision:  94.03%; recall:  96.72%; FB1:  95.36%\n",
      "              age: precision:  67.22%; recall:  89.44%; FB1:  76.76%  958\n",
      "             date: precision:  97.57%; recall:  98.18%; FB1:  97.88%  22264\n",
      "    hospital_name: precision:  59.06%; recall:  84.41%; FB1:  69.49%  1082\n",
      "               id: precision:  97.15%; recall:  97.70%; FB1:  97.42%  2138\n",
      "         location: precision:  90.61%; recall:  93.04%; FB1:  91.81%  1299\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  0\n",
      "      person_name: precision:  92.84%; recall:  95.47%; FB1:  94.13%  6282\n",
      "          room_no: precision:  75.83%; recall:  91.52%; FB1:  82.94%  484\n",
      "     telephone_no: precision:  76.47%; recall:  70.27%; FB1:  73.24%  68\n",
      "          web_url: precision:   0.00%; recall:   0.00%; FB1:   0.00%  0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   6%|▋         | 1/16 [1:09:15<17:18:50, 4155.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 12:08:35.403011 Step: 1 of 24172 Loss: 32.534004\n",
      "2019-12-12 12:11:09.041302 Step: 1000 of 24172 Loss: 0.219208\n",
      "2019-12-12 12:13:44.349842 Step: 2000 of 24172 Loss: 0.173935\n",
      "2019-12-12 12:16:17.768243 Step: 3000 of 24172 Loss: 0.205215\n",
      "2019-12-12 12:18:56.248459 Step: 4000 of 24172 Loss: 0.185801\n",
      "2019-12-12 12:21:31.193181 Step: 5000 of 24172 Loss: 0.211409\n",
      "2019-12-12 12:24:04.981140 Step: 6000 of 24172 Loss: 0.192493\n",
      "2019-12-12 12:26:38.440879 Step: 7000 of 24172 Loss: 0.206257\n",
      "2019-12-12 12:29:10.706288 Step: 8000 of 24172 Loss: 0.187813\n",
      "2019-12-12 12:31:44.421917 Step: 9000 of 24172 Loss: 0.173200\n",
      "2019-12-12 12:34:18.710279 Step: 10000 of 24172 Loss: 0.163917\n",
      "2019-12-12 12:36:51.717697 Step: 11000 of 24172 Loss: 0.183554\n",
      "2019-12-12 12:39:27.813054 Step: 12000 of 24172 Loss: 0.217657\n",
      "2019-12-12 12:42:04.576091 Step: 13000 of 24172 Loss: 0.178926\n",
      "2019-12-12 12:44:41.032077 Step: 14000 of 24172 Loss: 0.196423\n",
      "2019-12-12 12:47:18.637624 Step: 15000 of 24172 Loss: 0.188237\n",
      "2019-12-12 12:49:51.586251 Step: 16000 of 24172 Loss: 0.165741\n",
      "2019-12-12 12:52:25.399511 Step: 17000 of 24172 Loss: 0.245487\n",
      "2019-12-12 12:55:05.167638 Step: 18000 of 24172 Loss: 0.202668\n",
      "2019-12-12 12:57:39.934873 Step: 19000 of 24172 Loss: 0.177626\n",
      "2019-12-12 13:00:12.912791 Step: 20000 of 24172 Loss: 0.162527\n",
      "2019-12-12 13:02:50.382437 Step: 21000 of 24172 Loss: 0.191136\n",
      "2019-12-12 13:05:26.340399 Step: 22000 of 24172 Loss: 0.163316\n",
      "2019-12-12 13:07:56.945661 Step: 23000 of 24172 Loss: 0.182748\n",
      "2019-12-12 13:10:32.839652 Step: 24000 of 24172 Loss: 0.171881\n",
      "Training Loss: 0.188105 for epoch 1\n",
      "Epoch:  1\n",
      "processed 917637 tokens with 33613 phrases; found: 34007 phrases; correct: 32491.\n",
      "accuracy:  97.08%; (non-O)\n",
      "accuracy:  99.71%; precision:  95.54%; recall:  96.66%; FB1:  96.10%\n",
      "              age: precision:  74.70%; recall:  68.89%; FB1:  71.68%  664\n",
      "             date: precision:  97.61%; recall:  98.87%; FB1:  98.24%  22413\n",
      "    hospital_name: precision:  79.79%; recall:  80.85%; FB1:  80.31%  767\n",
      "               id: precision:  96.71%; recall:  98.02%; FB1:  97.36%  2155\n",
      "         location: precision:  90.81%; recall:  93.75%; FB1:  92.26%  1306\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  0\n",
      "      person_name: precision:  94.11%; recall:  94.66%; FB1:  94.39%  6145\n",
      "          room_no: precision:  86.28%; recall:  97.26%; FB1:  91.44%  452\n",
      "     telephone_no: precision:  72.00%; recall:  72.97%; FB1:  72.48%  75\n",
      "          web_url: precision:  26.67%; recall:  66.67%; FB1:  38.10%  30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|█▎        | 2/16 [2:18:27<16:09:21, 4154.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 13:17:47.700507 Step: 1 of 24172 Loss: 32.253393\n",
      "2019-12-12 13:20:20.333713 Step: 1000 of 24172 Loss: 0.180307\n",
      "2019-12-12 13:22:53.785101 Step: 2000 of 24172 Loss: 0.182796\n",
      "2019-12-12 13:25:28.450626 Step: 3000 of 24172 Loss: 0.158083\n",
      "2019-12-12 13:28:02.261470 Step: 4000 of 24172 Loss: 0.177619\n",
      "2019-12-12 13:30:37.120715 Step: 5000 of 24172 Loss: 0.141168\n",
      "2019-12-12 13:33:14.320097 Step: 6000 of 24172 Loss: 0.199302\n",
      "2019-12-12 13:35:51.343998 Step: 7000 of 24172 Loss: 0.141470\n",
      "2019-12-12 13:38:25.406830 Step: 8000 of 24172 Loss: 0.168327\n",
      "2019-12-12 13:41:01.657518 Step: 9000 of 24172 Loss: 0.215091\n",
      "2019-12-12 13:43:33.736927 Step: 10000 of 24172 Loss: 0.144494\n",
      "2019-12-12 13:46:09.083549 Step: 11000 of 24172 Loss: 0.144449\n",
      "2019-12-12 13:48:43.839487 Step: 12000 of 24172 Loss: 0.150707\n",
      "2019-12-12 13:51:21.708807 Step: 13000 of 24172 Loss: 0.179085\n",
      "2019-12-12 13:53:55.514838 Step: 14000 of 24172 Loss: 0.129555\n",
      "2019-12-12 13:56:25.344920 Step: 15000 of 24172 Loss: 0.151276\n",
      "2019-12-12 13:59:01.244816 Step: 16000 of 24172 Loss: 0.160614\n",
      "2019-12-12 14:01:36.713898 Step: 17000 of 24172 Loss: 0.160922\n",
      "2019-12-12 14:04:15.502430 Step: 18000 of 24172 Loss: 0.154764\n",
      "2019-12-12 14:06:50.132136 Step: 19000 of 24172 Loss: 0.152341\n",
      "2019-12-12 14:09:22.330244 Step: 20000 of 24172 Loss: 0.142124\n",
      "2019-12-12 14:11:56.238221 Step: 21000 of 24172 Loss: 0.153413\n",
      "2019-12-12 14:14:30.664513 Step: 22000 of 24172 Loss: 0.168652\n",
      "2019-12-12 14:17:07.002387 Step: 23000 of 24172 Loss: 0.148229\n",
      "2019-12-12 14:19:40.447888 Step: 24000 of 24172 Loss: 0.163116\n",
      "Training Loss: 0.159700 for epoch 2\n",
      "Epoch:  2\n",
      "processed 917637 tokens with 33613 phrases; found: 33984 phrases; correct: 32532.\n",
      "accuracy:  97.34%; (non-O)\n",
      "accuracy:  99.72%; precision:  95.73%; recall:  96.78%; FB1:  96.25%\n",
      "              age: precision:  74.90%; recall:  80.00%; FB1:  77.37%  769\n",
      "             date: precision:  97.53%; recall:  98.92%; FB1:  98.22%  22444\n",
      "    hospital_name: precision:  82.89%; recall:  81.90%; FB1:  82.39%  748\n",
      "               id: precision:  98.52%; recall:  97.22%; FB1:  97.87%  2098\n",
      "         location: precision:  92.21%; recall:  92.65%; FB1:  92.43%  1271\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  0\n",
      "      person_name: precision:  93.70%; recall:  94.91%; FB1:  94.30%  6188\n",
      "          room_no: precision:  89.73%; recall:  91.52%; FB1:  90.62%  409\n",
      "     telephone_no: precision:  75.44%; recall:  58.11%; FB1:  65.65%  57\n",
      "          web_url: precision:   0.00%; recall:   0.00%; FB1:   0.00%  0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  19%|█▉        | 3/16 [3:27:36<14:59:46, 4152.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 14:26:56.681542 Step: 1 of 24172 Loss: 24.593549\n",
      "2019-12-12 14:29:33.745244 Step: 1000 of 24172 Loss: 0.201912\n",
      "2019-12-12 14:32:14.536467 Step: 2000 of 24172 Loss: 0.135263\n",
      "2019-12-12 14:34:48.384429 Step: 3000 of 24172 Loss: 0.130110\n",
      "2019-12-12 14:37:21.060845 Step: 4000 of 24172 Loss: 0.134910\n",
      "2019-12-12 14:39:55.095016 Step: 5000 of 24172 Loss: 0.126409\n",
      "2019-12-12 14:42:26.493416 Step: 6000 of 24172 Loss: 0.135466\n",
      "2019-12-12 14:44:59.936195 Step: 7000 of 24172 Loss: 0.171249\n",
      "2019-12-12 14:47:36.751453 Step: 8000 of 24172 Loss: 0.136760\n",
      "2019-12-12 14:50:12.494865 Step: 9000 of 24172 Loss: 0.148891\n",
      "2019-12-12 14:52:41.914334 Step: 10000 of 24172 Loss: 0.135607\n",
      "2019-12-12 14:55:14.233827 Step: 11000 of 24172 Loss: 0.126229\n",
      "2019-12-12 14:57:44.714385 Step: 12000 of 24172 Loss: 0.124861\n",
      "2019-12-12 15:00:22.291663 Step: 13000 of 24172 Loss: 0.136952\n",
      "2019-12-12 15:02:56.120941 Step: 14000 of 24172 Loss: 0.136275\n",
      "2019-12-12 15:05:32.874807 Step: 15000 of 24172 Loss: 0.134488\n",
      "2019-12-12 15:08:09.400701 Step: 16000 of 24172 Loss: 0.159707\n",
      "2019-12-12 15:10:40.647595 Step: 17000 of 24172 Loss: 0.152918\n",
      "2019-12-12 15:13:16.377924 Step: 18000 of 24172 Loss: 0.133323\n",
      "2019-12-12 15:15:52.168406 Step: 19000 of 24172 Loss: 0.161612\n",
      "2019-12-12 15:18:28.592576 Step: 20000 of 24172 Loss: 0.123326\n",
      "2019-12-12 15:21:04.902682 Step: 21000 of 24172 Loss: 0.132513\n",
      "2019-12-12 15:23:42.346055 Step: 22000 of 24172 Loss: 0.139946\n",
      "2019-12-12 15:26:20.083405 Step: 23000 of 24172 Loss: 0.195249\n",
      "2019-12-12 15:28:53.156903 Step: 24000 of 24172 Loss: 0.135658\n",
      "Training Loss: 0.142666 for epoch 3\n",
      "Epoch:  3\n",
      "processed 917637 tokens with 33613 phrases; found: 34270 phrases; correct: 32580.\n",
      "accuracy:  97.47%; (non-O)\n",
      "accuracy:  99.71%; precision:  95.07%; recall:  96.93%; FB1:  95.99%\n",
      "              age: precision:  69.31%; recall:  86.25%; FB1:  76.86%  896\n",
      "             date: precision:  97.57%; recall:  98.87%; FB1:  98.22%  22420\n",
      "    hospital_name: precision:  72.86%; recall:  87.58%; FB1:  79.54%  910\n",
      "               id: precision:  97.77%; recall:  96.75%; FB1:  97.26%  2104\n",
      "         location: precision:  96.63%; recall:  88.46%; FB1:  92.36%  1158\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  0\n",
      "      person_name: precision:  92.48%; recall:  95.19%; FB1:  93.81%  6288\n",
      "          room_no: precision:  89.05%; recall:  93.27%; FB1:  91.11%  420\n",
      "     telephone_no: precision:  77.46%; recall:  74.32%; FB1:  75.86%  71\n",
      "          web_url: precision:   0.00%; recall:   0.00%; FB1:   0.00%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 4/16 [4:36:46<13:50:21, 4151.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 15:36:06.169704 Step: 1 of 24172 Loss: 23.590526\n",
      "2019-12-12 15:38:43.386451 Step: 1000 of 24172 Loss: 0.150444\n",
      "2019-12-12 15:41:16.023924 Step: 2000 of 24172 Loss: 0.139901\n",
      "2019-12-12 15:43:52.099036 Step: 3000 of 24172 Loss: 0.121515\n",
      "2019-12-12 15:46:22.789718 Step: 4000 of 24172 Loss: 0.110346\n",
      "2019-12-12 15:48:58.851253 Step: 5000 of 24172 Loss: 0.109361\n",
      "2019-12-12 15:51:34.255484 Step: 6000 of 24172 Loss: 0.131733\n",
      "2019-12-12 15:54:07.232409 Step: 7000 of 24172 Loss: 0.110960\n",
      "2019-12-12 15:56:43.546356 Step: 8000 of 24172 Loss: 0.125873\n",
      "2019-12-12 15:59:17.757369 Step: 9000 of 24172 Loss: 0.119122\n",
      "2019-12-12 16:01:52.594569 Step: 10000 of 24172 Loss: 0.127909\n",
      "2019-12-12 16:04:28.981250 Step: 11000 of 24172 Loss: 0.139870\n",
      "2019-12-12 16:07:04.389063 Step: 12000 of 24172 Loss: 0.137206\n",
      "2019-12-12 16:09:38.583331 Step: 13000 of 24172 Loss: 0.128692\n",
      "2019-12-12 16:12:12.133003 Step: 14000 of 24172 Loss: 0.137129\n",
      "2019-12-12 16:14:49.017422 Step: 15000 of 24172 Loss: 0.132328\n",
      "2019-12-12 16:17:25.407261 Step: 16000 of 24172 Loss: 0.155234\n",
      "2019-12-12 16:19:58.195365 Step: 17000 of 24172 Loss: 0.114012\n",
      "2019-12-12 16:22:37.364325 Step: 18000 of 24172 Loss: 0.133563\n",
      "2019-12-12 16:25:10.324069 Step: 19000 of 24172 Loss: 0.129943\n",
      "2019-12-12 16:27:48.194138 Step: 20000 of 24172 Loss: 0.125260\n",
      "2019-12-12 16:30:23.950774 Step: 21000 of 24172 Loss: 0.108066\n",
      "2019-12-12 16:32:58.870570 Step: 22000 of 24172 Loss: 0.124589\n",
      "2019-12-12 16:35:33.132379 Step: 23000 of 24172 Loss: 0.143088\n",
      "2019-12-12 16:38:04.296181 Step: 24000 of 24172 Loss: 0.142618\n",
      "Training Loss: 0.128756 for epoch 4\n",
      "Epoch:  4\n",
      "processed 917637 tokens with 33613 phrases; found: 33852 phrases; correct: 32445.\n",
      "accuracy:  96.86%; (non-O)\n",
      "accuracy:  99.71%; precision:  95.84%; recall:  96.53%; FB1:  96.18%\n",
      "              age: precision:  77.36%; recall:  79.72%; FB1:  78.52%  742\n",
      "             date: precision:  97.89%; recall:  98.54%; FB1:  98.21%  22276\n",
      "    hospital_name: precision:  78.86%; recall:  85.73%; FB1:  82.15%  823\n",
      "               id: precision:  97.77%; recall:  96.94%; FB1:  97.35%  2108\n",
      "         location: precision:  92.10%; recall:  94.94%; FB1:  93.50%  1304\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  0\n",
      "      person_name: precision:  94.10%; recall:  93.94%; FB1:  94.02%  6099\n",
      "          room_no: precision:  86.04%; recall:  95.26%; FB1:  90.41%  444\n",
      "     telephone_no: precision:  60.38%; recall:  43.24%; FB1:  50.39%  53\n",
      "          web_url: precision:  66.67%; recall:  16.67%; FB1:  26.67%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  31%|███▏      | 5/16 [5:45:57<12:41:09, 4151.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 16:45:17.855196 Step: 1 of 24172 Loss: 37.147682\n",
      "2019-12-12 16:47:54.639727 Step: 1000 of 24172 Loss: 0.154548\n",
      "2019-12-12 16:50:28.489595 Step: 2000 of 24172 Loss: 0.102568\n",
      "2019-12-12 16:53:05.844950 Step: 3000 of 24172 Loss: 0.108817\n",
      "2019-12-12 16:55:45.217001 Step: 4000 of 24172 Loss: 0.104938\n",
      "2019-12-12 16:58:18.937216 Step: 5000 of 24172 Loss: 0.125176\n",
      "2019-12-12 17:00:53.828220 Step: 6000 of 24172 Loss: 0.121436\n",
      "2019-12-12 17:03:27.871563 Step: 7000 of 24172 Loss: 0.125761\n",
      "2019-12-12 17:06:03.885038 Step: 8000 of 24172 Loss: 0.154288\n",
      "2019-12-12 17:08:36.848015 Step: 9000 of 24172 Loss: 0.116299\n",
      "2019-12-12 17:11:13.530678 Step: 10000 of 24172 Loss: 0.139283\n",
      "2019-12-12 17:13:47.050634 Step: 11000 of 24172 Loss: 0.119449\n",
      "2019-12-12 17:16:19.339307 Step: 12000 of 24172 Loss: 0.105658\n",
      "2019-12-12 17:18:55.298784 Step: 13000 of 24172 Loss: 0.214857\n",
      "2019-12-12 17:21:32.325753 Step: 14000 of 24172 Loss: 0.159763\n",
      "2019-12-12 17:24:08.612756 Step: 15000 of 24172 Loss: 0.114172\n",
      "2019-12-12 17:26:42.369227 Step: 16000 of 24172 Loss: 0.104421\n",
      "2019-12-12 17:29:16.452043 Step: 17000 of 24172 Loss: 0.113296\n",
      "2019-12-12 17:31:51.653775 Step: 18000 of 24172 Loss: 0.111073\n",
      "2019-12-12 17:34:24.312588 Step: 19000 of 24172 Loss: 0.111411\n",
      "2019-12-12 17:36:59.198904 Step: 20000 of 24172 Loss: 0.111595\n",
      "2019-12-12 17:39:35.609672 Step: 21000 of 24172 Loss: 0.134504\n",
      "2019-12-12 17:42:08.950950 Step: 22000 of 24172 Loss: 0.102996\n",
      "2019-12-12 17:44:42.278918 Step: 23000 of 24172 Loss: 0.110520\n",
      "2019-12-12 17:47:15.658922 Step: 24000 of 24172 Loss: 0.113656\n",
      "Training Loss: 0.122589 for epoch 5\n",
      "Epoch:  5\n",
      "processed 917637 tokens with 33613 phrases; found: 33888 phrases; correct: 32499.\n",
      "accuracy:  97.26%; (non-O)\n",
      "accuracy:  99.72%; precision:  95.90%; recall:  96.69%; FB1:  96.29%\n",
      "              age: precision:  81.76%; recall:  69.72%; FB1:  75.26%  614\n",
      "             date: precision:  97.69%; recall:  98.67%; FB1:  98.18%  22350\n",
      "    hospital_name: precision:  79.63%; recall:  85.20%; FB1:  82.32%  810\n",
      "               id: precision:  98.43%; recall:  97.55%; FB1:  97.99%  2107\n",
      "         location: precision:  93.34%; recall:  92.02%; FB1:  92.68%  1247\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  1\n",
      "      person_name: precision:  93.70%; recall:  95.69%; FB1:  94.69%  6239\n",
      "          room_no: precision:  88.94%; recall:  94.26%; FB1:  91.53%  425\n",
      "     telephone_no: precision:  65.28%; recall:  63.51%; FB1:  64.38%  72\n",
      "          web_url: precision:  43.48%; recall:  83.33%; FB1:  57.14%  23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  38%|███▊      | 6/16 [6:55:09<11:31:57, 4151.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 17:54:29.554759 Step: 1 of 24172 Loss: 19.913289\n",
      "2019-12-12 17:57:04.861862 Step: 1000 of 24172 Loss: 0.113027\n",
      "2019-12-12 17:59:38.251967 Step: 2000 of 24172 Loss: 0.088763\n",
      "2019-12-12 18:02:12.092026 Step: 3000 of 24172 Loss: 0.094553\n",
      "2019-12-12 18:04:47.526738 Step: 4000 of 24172 Loss: 0.106658\n",
      "2019-12-12 18:07:27.516717 Step: 5000 of 24172 Loss: 0.110471\n",
      "2019-12-12 18:09:59.955374 Step: 6000 of 24172 Loss: 0.090663\n",
      "2019-12-12 18:12:35.665944 Step: 7000 of 24172 Loss: 0.094827\n",
      "2019-12-12 18:15:08.431227 Step: 8000 of 24172 Loss: 0.115780\n",
      "2019-12-12 18:17:44.340766 Step: 9000 of 24172 Loss: 0.101459\n",
      "2019-12-12 18:20:18.536640 Step: 10000 of 24172 Loss: 0.104792\n",
      "2019-12-12 18:22:49.601221 Step: 11000 of 24172 Loss: 0.095912\n",
      "2019-12-12 18:25:24.830923 Step: 12000 of 24172 Loss: 0.090197\n",
      "2019-12-12 18:28:00.577945 Step: 13000 of 24172 Loss: 0.092323\n",
      "2019-12-12 18:30:38.102603 Step: 14000 of 24172 Loss: 0.101155\n",
      "2019-12-12 18:33:16.436006 Step: 15000 of 24172 Loss: 0.138219\n",
      "2019-12-12 18:35:47.057119 Step: 16000 of 24172 Loss: 0.101997\n",
      "2019-12-12 18:38:24.780134 Step: 17000 of 24172 Loss: 0.099890\n",
      "2019-12-12 18:41:00.198151 Step: 18000 of 24172 Loss: 0.103356\n",
      "2019-12-12 18:43:32.821120 Step: 19000 of 24172 Loss: 0.114237\n",
      "2019-12-12 18:46:07.600089 Step: 20000 of 24172 Loss: 0.109054\n",
      "2019-12-12 18:48:42.406227 Step: 21000 of 24172 Loss: 0.104942\n",
      "2019-12-12 18:51:13.936456 Step: 22000 of 24172 Loss: 0.108083\n",
      "2019-12-12 18:53:52.933801 Step: 23000 of 24172 Loss: 0.136239\n",
      "2019-12-12 18:56:29.736329 Step: 24000 of 24172 Loss: 0.106667\n",
      "Training Loss: 0.104346 for epoch 6\n",
      "Epoch:  6\n",
      "processed 917637 tokens with 33613 phrases; found: 34201 phrases; correct: 32633.\n",
      "accuracy:  97.41%; (non-O)\n",
      "accuracy:  99.72%; precision:  95.42%; recall:  97.08%; FB1:  96.24%\n",
      "              age: precision:  74.16%; recall:  82.50%; FB1:  78.11%  801\n",
      "             date: precision:  97.74%; recall:  98.89%; FB1:  98.31%  22388\n",
      "    hospital_name: precision:  77.89%; recall:  88.90%; FB1:  83.04%  864\n",
      "               id: precision:  98.16%; recall:  97.74%; FB1:  97.95%  2117\n",
      "         location: precision:  92.70%; recall:  92.33%; FB1:  92.51%  1260\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  2\n",
      "      person_name: precision:  93.10%; recall:  94.68%; FB1:  93.88%  6213\n",
      "          room_no: precision:  84.78%; recall:  97.26%; FB1:  90.59%  460\n",
      "     telephone_no: precision:  75.34%; recall:  74.32%; FB1:  74.83%  73\n",
      "          web_url: precision:  43.48%; recall:  83.33%; FB1:  57.14%  23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  44%|████▍     | 7/16 [8:04:23<10:22:50, 4152.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 19:03:43.269058 Step: 1 of 24172 Loss: 18.742744\n",
      "2019-12-12 19:06:16.415018 Step: 1000 of 24172 Loss: 0.098921\n",
      "2019-12-12 19:08:50.557509 Step: 2000 of 24172 Loss: 0.079359\n",
      "2019-12-12 19:11:27.566978 Step: 3000 of 24172 Loss: 0.097570\n",
      "2019-12-12 19:14:02.024074 Step: 4000 of 24172 Loss: 0.084051\n",
      "2019-12-12 19:16:37.679980 Step: 5000 of 24172 Loss: 0.099327\n",
      "2019-12-12 19:19:11.343634 Step: 6000 of 24172 Loss: 0.081593\n",
      "2019-12-12 19:21:45.755197 Step: 7000 of 24172 Loss: 0.089562\n",
      "2019-12-12 19:24:18.835892 Step: 8000 of 24172 Loss: 0.099963\n",
      "2019-12-12 19:26:55.000570 Step: 9000 of 24172 Loss: 0.102430\n",
      "2019-12-12 19:29:28.570487 Step: 10000 of 24172 Loss: 0.098159\n",
      "2019-12-12 19:32:03.607158 Step: 11000 of 24172 Loss: 0.090958\n",
      "2019-12-12 19:34:40.455972 Step: 12000 of 24172 Loss: 0.095449\n",
      "2019-12-12 19:37:17.846180 Step: 13000 of 24172 Loss: 0.087658\n",
      "2019-12-12 19:39:54.301500 Step: 14000 of 24172 Loss: 0.101100\n",
      "2019-12-12 19:42:29.703484 Step: 15000 of 24172 Loss: 0.083376\n",
      "2019-12-12 19:45:04.271025 Step: 16000 of 24172 Loss: 0.086995\n",
      "2019-12-12 19:47:42.238628 Step: 17000 of 24172 Loss: 0.087888\n",
      "2019-12-12 19:50:18.315638 Step: 18000 of 24172 Loss: 0.099738\n",
      "2019-12-12 19:52:53.549926 Step: 19000 of 24172 Loss: 0.094206\n",
      "2019-12-12 19:55:27.296128 Step: 20000 of 24172 Loss: 0.088574\n",
      "2019-12-12 19:58:04.068218 Step: 21000 of 24172 Loss: 0.095571\n",
      "2019-12-12 20:00:35.697577 Step: 22000 of 24172 Loss: 0.101864\n",
      "2019-12-12 20:03:09.453687 Step: 23000 of 24172 Loss: 0.093861\n",
      "2019-12-12 20:05:43.529220 Step: 24000 of 24172 Loss: 0.085522\n",
      "Training Loss: 0.092397 for epoch 7\n",
      "Epoch:  7\n",
      "processed 917637 tokens with 33613 phrases; found: 33902 phrases; correct: 32516.\n",
      "accuracy:  97.35%; (non-O)\n",
      "accuracy:  99.73%; precision:  95.91%; recall:  96.74%; FB1:  96.32%\n",
      "              age: precision:  80.06%; recall:  77.50%; FB1:  78.76%  697\n",
      "             date: precision:  97.79%; recall:  98.74%; FB1:  98.26%  22342\n",
      "    hospital_name: precision:  82.09%; recall:  84.15%; FB1:  83.11%  776\n",
      "               id: precision:  98.34%; recall:  97.37%; FB1:  97.85%  2105\n",
      "         location: precision:  93.54%; recall:  92.73%; FB1:  93.13%  1254\n",
      "         org_name: precision:   9.09%; recall:   4.55%; FB1:   6.06%  11\n",
      "      person_name: precision:  93.18%; recall:  95.01%; FB1:  94.08%  6229\n",
      "          room_no: precision:  88.89%; recall:  93.77%; FB1:  91.26%  423\n",
      "     telephone_no: precision:  79.03%; recall:  66.22%; FB1:  72.06%  62\n",
      "          web_url: precision:   0.00%; recall:   0.00%; FB1:   0.00%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 8/16 [9:13:37<9:13:42, 4152.82s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 20:12:57.179536 Step: 1 of 24172 Loss: 28.533051\n",
      "2019-12-12 20:15:31.526492 Step: 1000 of 24172 Loss: 0.102372\n",
      "2019-12-12 20:18:05.363912 Step: 2000 of 24172 Loss: 0.078202\n",
      "2019-12-12 20:20:43.198231 Step: 3000 of 24172 Loss: 0.100610\n",
      "2019-12-12 20:23:17.340620 Step: 4000 of 24172 Loss: 0.070981\n",
      "2019-12-12 20:25:52.843869 Step: 5000 of 24172 Loss: 0.081667\n",
      "2019-12-12 20:28:29.883514 Step: 6000 of 24172 Loss: 0.077296\n",
      "2019-12-12 20:31:02.323598 Step: 7000 of 24172 Loss: 0.078350\n",
      "2019-12-12 20:33:36.803332 Step: 8000 of 24172 Loss: 0.078822\n",
      "2019-12-12 20:36:06.960031 Step: 9000 of 24172 Loss: 0.083933\n",
      "2019-12-12 20:38:41.817193 Step: 10000 of 24172 Loss: 0.082574\n",
      "2019-12-12 20:41:17.425839 Step: 11000 of 24172 Loss: 0.076053\n",
      "2019-12-12 20:43:52.219380 Step: 12000 of 24172 Loss: 0.086875\n",
      "2019-12-12 20:46:30.332380 Step: 13000 of 24172 Loss: 0.074746\n",
      "2019-12-12 20:49:02.665050 Step: 14000 of 24172 Loss: 0.084025\n",
      "2019-12-12 20:51:37.909000 Step: 15000 of 24172 Loss: 0.078100\n",
      "2019-12-12 20:54:13.336811 Step: 16000 of 24172 Loss: 0.070132\n",
      "2019-12-12 20:56:49.059050 Step: 17000 of 24172 Loss: 0.077572\n",
      "2019-12-12 20:59:28.610214 Step: 18000 of 24172 Loss: 0.082494\n",
      "2019-12-12 21:02:00.218159 Step: 19000 of 24172 Loss: 0.081034\n",
      "2019-12-12 21:04:36.718157 Step: 20000 of 24172 Loss: 0.077472\n",
      "2019-12-12 21:07:09.724359 Step: 21000 of 24172 Loss: 0.071788\n",
      "2019-12-12 21:09:44.123685 Step: 22000 of 24172 Loss: 0.086925\n",
      "2019-12-12 21:12:18.572951 Step: 23000 of 24172 Loss: 0.085898\n",
      "2019-12-12 21:14:55.964253 Step: 24000 of 24172 Loss: 0.088383\n",
      "Training Loss: 0.080382 for epoch 8\n",
      "Epoch:  8\n",
      "processed 917637 tokens with 33613 phrases; found: 34151 phrases; correct: 32636.\n",
      "accuracy:  97.55%; (non-O)\n",
      "accuracy:  99.73%; precision:  95.56%; recall:  97.09%; FB1:  96.32%\n",
      "              age: precision:  76.12%; recall:  82.78%; FB1:  79.31%  783\n",
      "             date: precision:  97.70%; recall:  98.83%; FB1:  98.27%  22383\n",
      "    hospital_name: precision:  81.50%; recall:  84.94%; FB1:  83.18%  789\n",
      "               id: precision:  98.43%; recall:  97.22%; FB1:  97.82%  2100\n",
      "         location: precision:  93.60%; recall:  93.60%; FB1:  93.60%  1265\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  4\n",
      "      person_name: precision:  92.50%; recall:  95.47%; FB1:  93.96%  6305\n",
      "          room_no: precision:  87.16%; recall:  96.51%; FB1:  91.60%  444\n",
      "     telephone_no: precision:  74.67%; recall:  75.68%; FB1:  75.17%  75\n",
      "          web_url: precision:  66.67%; recall:  16.67%; FB1:  26.67%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  56%|█████▋    | 9/16 [10:22:49<8:04:27, 4152.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 21:22:09.071406 Step: 1 of 24172 Loss: 15.153591\n",
      "2019-12-12 21:24:47.799929 Step: 1000 of 24172 Loss: 0.079918\n",
      "2019-12-12 21:27:22.487228 Step: 2000 of 24172 Loss: 0.072828\n",
      "2019-12-12 21:29:57.578823 Step: 3000 of 24172 Loss: 0.062885\n",
      "2019-12-12 21:32:34.026754 Step: 4000 of 24172 Loss: 0.078594\n",
      "2019-12-12 21:35:09.767516 Step: 5000 of 24172 Loss: 0.072612\n",
      "2019-12-12 21:37:41.335003 Step: 6000 of 24172 Loss: 0.071626\n",
      "2019-12-12 21:40:16.354176 Step: 7000 of 24172 Loss: 0.072785\n",
      "2019-12-12 21:42:50.795176 Step: 8000 of 24172 Loss: 0.063933\n",
      "2019-12-12 21:45:26.617433 Step: 9000 of 24172 Loss: 0.069895\n",
      "2019-12-12 21:48:00.005905 Step: 10000 of 24172 Loss: 0.069034\n",
      "2019-12-12 21:50:36.288122 Step: 11000 of 24172 Loss: 0.071193\n",
      "2019-12-12 21:53:11.469035 Step: 12000 of 24172 Loss: 0.057943\n",
      "2019-12-12 21:55:47.562404 Step: 13000 of 24172 Loss: 0.074424\n",
      "2019-12-12 21:58:22.848960 Step: 14000 of 24172 Loss: 0.071314\n",
      "2019-12-12 22:00:58.761284 Step: 15000 of 24172 Loss: 0.072267\n",
      "2019-12-12 22:03:33.195132 Step: 16000 of 24172 Loss: 0.068870\n",
      "2019-12-12 22:06:07.421843 Step: 17000 of 24172 Loss: 0.063672\n",
      "2019-12-12 22:08:39.487684 Step: 18000 of 24172 Loss: 0.064579\n",
      "2019-12-12 22:11:15.728685 Step: 19000 of 24172 Loss: 0.066445\n",
      "2019-12-12 22:13:52.024018 Step: 20000 of 24172 Loss: 0.067251\n",
      "2019-12-12 22:16:25.361657 Step: 21000 of 24172 Loss: 0.075045\n",
      "2019-12-12 22:18:58.669019 Step: 22000 of 24172 Loss: 0.057916\n",
      "2019-12-12 22:21:33.325515 Step: 23000 of 24172 Loss: 0.112379\n",
      "2019-12-12 22:24:05.952795 Step: 24000 of 24172 Loss: 0.077342\n",
      "Training Loss: 0.070828 for epoch 9\n",
      "Epoch:  9\n",
      "processed 917637 tokens with 33613 phrases; found: 33849 phrases; correct: 32552.\n",
      "accuracy:  97.29%; (non-O)\n",
      "accuracy:  99.74%; precision:  96.17%; recall:  96.84%; FB1:  96.50%\n",
      "              age: precision:  77.93%; recall:  80.42%; FB1:  79.15%  743\n",
      "             date: precision:  98.09%; recall:  98.92%; FB1:  98.50%  22314\n",
      "    hospital_name: precision:  83.65%; recall:  82.43%; FB1:  83.03%  746\n",
      "               id: precision:  98.43%; recall:  97.60%; FB1:  98.02%  2108\n",
      "         location: precision:  93.60%; recall:  93.68%; FB1:  93.64%  1266\n",
      "         org_name: precision:  50.00%; recall:   9.09%; FB1:  15.38%  4\n",
      "      person_name: precision:  93.48%; recall:  94.29%; FB1:  93.88%  6162\n",
      "          room_no: precision:  88.43%; recall:  95.26%; FB1:  91.72%  432\n",
      "     telephone_no: precision:  78.87%; recall:  75.68%; FB1:  77.24%  71\n",
      "          web_url: precision:  66.67%; recall:  16.67%; FB1:  26.67%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|██████▎   | 10/16 [11:31:58<6:55:10, 4151.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 22:31:18.915124 Step: 1 of 24172 Loss: 12.460103\n",
      "2019-12-12 22:33:53.506548 Step: 1000 of 24172 Loss: 0.081553\n",
      "2019-12-12 22:36:25.519036 Step: 2000 of 24172 Loss: 0.057688\n",
      "2019-12-12 22:38:56.764387 Step: 3000 of 24172 Loss: 0.052710\n",
      "2019-12-12 22:41:31.398694 Step: 4000 of 24172 Loss: 0.062250\n",
      "2019-12-12 22:44:08.059009 Step: 5000 of 24172 Loss: 0.055657\n",
      "2019-12-12 22:46:44.741188 Step: 6000 of 24172 Loss: 0.070415\n",
      "2019-12-12 22:49:20.986775 Step: 7000 of 24172 Loss: 0.067127\n",
      "2019-12-12 22:52:00.703261 Step: 8000 of 24172 Loss: 0.066107\n",
      "2019-12-12 22:54:36.293331 Step: 9000 of 24172 Loss: 0.055861\n",
      "2019-12-12 22:57:08.911212 Step: 10000 of 24172 Loss: 0.064632\n",
      "2019-12-12 22:59:42.619641 Step: 11000 of 24172 Loss: 0.058871\n",
      "2019-12-12 23:02:17.486136 Step: 12000 of 24172 Loss: 0.054384\n",
      "2019-12-12 23:04:55.631424 Step: 13000 of 24172 Loss: 0.064047\n",
      "2019-12-12 23:07:34.745464 Step: 14000 of 24172 Loss: 0.058128\n",
      "2019-12-12 23:10:03.542449 Step: 15000 of 24172 Loss: 0.060379\n",
      "2019-12-12 23:12:36.034325 Step: 16000 of 24172 Loss: 0.056620\n",
      "2019-12-12 23:15:06.207627 Step: 17000 of 24172 Loss: 0.062167\n",
      "2019-12-12 23:17:38.954430 Step: 18000 of 24172 Loss: 0.056727\n",
      "2019-12-12 23:20:14.892403 Step: 19000 of 24172 Loss: 0.060537\n",
      "2019-12-12 23:22:52.581353 Step: 20000 of 24172 Loss: 0.067466\n",
      "2019-12-12 23:25:28.467446 Step: 21000 of 24172 Loss: 0.053846\n",
      "2019-12-12 23:28:03.465531 Step: 22000 of 24172 Loss: 0.059103\n",
      "2019-12-12 23:30:40.789964 Step: 23000 of 24172 Loss: 0.067254\n",
      "2019-12-12 23:33:14.128399 Step: 24000 of 24172 Loss: 0.056005\n",
      "Training Loss: 0.060792 for epoch 10\n",
      "Epoch:  10\n",
      "processed 917637 tokens with 33613 phrases; found: 33942 phrases; correct: 32549.\n",
      "accuracy:  97.28%; (non-O)\n",
      "accuracy:  99.72%; precision:  95.90%; recall:  96.83%; FB1:  96.36%\n",
      "              age: precision:  70.05%; recall:  83.47%; FB1:  76.17%  858\n",
      "             date: precision:  98.14%; recall:  98.71%; FB1:  98.42%  22256\n",
      "    hospital_name: precision:  83.18%; recall:  84.94%; FB1:  84.05%  773\n",
      "               id: precision:  98.71%; recall:  97.22%; FB1:  97.96%  2094\n",
      "         location: precision:  92.78%; recall:  93.44%; FB1:  93.11%  1274\n",
      "         org_name: precision:   0.00%; recall:   0.00%; FB1:   0.00%  8\n",
      "      person_name: precision:  93.55%; recall:  94.55%; FB1:  94.05%  6174\n",
      "          room_no: precision:  88.71%; recall:  94.01%; FB1:  91.28%  425\n",
      "     telephone_no: precision:  76.62%; recall:  79.73%; FB1:  78.15%  77\n",
      "          web_url: precision: 100.00%; recall:  25.00%; FB1:  40.00%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  69%|██████▉   | 11/16 [12:41:07<5:45:54, 4150.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 23:40:27.826490 Step: 1 of 24172 Loss: 12.400414\n",
      "2019-12-12 23:43:02.082465 Step: 1000 of 24172 Loss: 0.060028\n",
      "2019-12-12 23:45:34.638174 Step: 2000 of 24172 Loss: 0.048213\n",
      "2019-12-12 23:48:12.308360 Step: 3000 of 24172 Loss: 0.047378\n",
      "2019-12-12 23:50:48.510472 Step: 4000 of 24172 Loss: 0.050644\n",
      "2019-12-12 23:53:22.399888 Step: 5000 of 24172 Loss: 0.045627\n",
      "2019-12-12 23:55:57.764302 Step: 6000 of 24172 Loss: 0.056117\n",
      "2019-12-12 23:58:33.525224 Step: 7000 of 24172 Loss: 0.062075\n",
      "2019-12-13 00:01:06.877742 Step: 8000 of 24172 Loss: 0.053602\n",
      "2019-12-13 00:03:40.016789 Step: 9000 of 24172 Loss: 0.054820\n",
      "2019-12-13 00:06:14.888300 Step: 10000 of 24172 Loss: 0.047147\n",
      "2019-12-13 00:08:51.679217 Step: 11000 of 24172 Loss: 0.046728\n",
      "2019-12-13 00:11:28.026236 Step: 12000 of 24172 Loss: 0.049648\n",
      "2019-12-13 00:14:05.563714 Step: 13000 of 24172 Loss: 0.050167\n",
      "2019-12-13 00:16:42.116876 Step: 14000 of 24172 Loss: 0.055489\n",
      "2019-12-13 00:19:15.426814 Step: 15000 of 24172 Loss: 0.060724\n",
      "2019-12-13 00:21:52.574817 Step: 16000 of 24172 Loss: 0.059009\n",
      "2019-12-13 00:24:23.472759 Step: 17000 of 24172 Loss: 0.057377\n",
      "2019-12-13 00:26:58.060881 Step: 18000 of 24172 Loss: 0.045998\n",
      "2019-12-13 00:29:31.885514 Step: 19000 of 24172 Loss: 0.052572\n",
      "2019-12-13 00:32:09.282280 Step: 20000 of 24172 Loss: 0.055290\n",
      "2019-12-13 00:34:43.841330 Step: 21000 of 24172 Loss: 0.096192\n",
      "2019-12-13 00:37:17.777362 Step: 22000 of 24172 Loss: 0.048935\n",
      "2019-12-13 00:39:52.573918 Step: 23000 of 24172 Loss: 0.061012\n",
      "2019-12-13 00:42:26.242064 Step: 24000 of 24172 Loss: 0.047683\n",
      "Training Loss: 0.054215 for epoch 11\n",
      "Epoch:  11\n",
      "processed 917637 tokens with 33613 phrases; found: 33905 phrases; correct: 32572.\n",
      "accuracy:  97.34%; (non-O)\n",
      "accuracy:  99.73%; precision:  96.07%; recall:  96.90%; FB1:  96.48%\n",
      "              age: precision:  75.78%; recall:  83.89%; FB1:  79.63%  797\n",
      "             date: precision:  97.97%; recall:  98.86%; FB1:  98.41%  22327\n",
      "    hospital_name: precision:  78.98%; recall:  87.85%; FB1:  83.18%  842\n",
      "               id: precision:  98.52%; recall:  96.85%; FB1:  97.68%  2090\n",
      "         location: precision:  95.51%; recall:  92.57%; FB1:  94.02%  1226\n",
      "         org_name: precision:  20.00%; recall:   9.09%; FB1:  12.50%  10\n",
      "      person_name: precision:  94.22%; recall:  94.40%; FB1:  94.31%  6121\n",
      "          room_no: precision:  89.07%; recall:  93.52%; FB1:  91.24%  421\n",
      "     telephone_no: precision:  76.47%; recall:  70.27%; FB1:  73.24%  68\n",
      "          web_url: precision: 100.00%; recall:  25.00%; FB1:  40.00%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 12/16 [13:50:23<4:36:49, 4152.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-13 00:49:43.799179 Step: 1 of 24172 Loss: 10.401160\n",
      "2019-12-13 00:52:18.678754 Step: 1000 of 24172 Loss: 0.054495\n",
      "2019-12-13 00:54:57.768134 Step: 2000 of 24172 Loss: 0.044199\n",
      "2019-12-13 00:57:29.912765 Step: 3000 of 24172 Loss: 0.045002\n",
      "2019-12-13 01:00:03.001100 Step: 4000 of 24172 Loss: 0.045109\n",
      "2019-12-13 01:02:40.282499 Step: 5000 of 24172 Loss: 0.035805\n",
      "2019-12-13 01:05:15.603954 Step: 6000 of 24172 Loss: 0.047305\n",
      "2019-12-13 01:07:54.076017 Step: 7000 of 24172 Loss: 0.045313\n",
      "2019-12-13 01:10:25.144164 Step: 8000 of 24172 Loss: 0.046791\n",
      "2019-12-13 01:13:00.886036 Step: 9000 of 24172 Loss: 0.040886\n",
      "2019-12-13 01:15:34.741527 Step: 10000 of 24172 Loss: 0.040070\n",
      "2019-12-13 01:18:07.888053 Step: 11000 of 24172 Loss: 0.053679\n",
      "2019-12-13 01:20:42.382181 Step: 12000 of 24172 Loss: 0.050979\n",
      "2019-12-13 01:23:15.717180 Step: 13000 of 24172 Loss: 0.048438\n",
      "2019-12-13 01:25:48.023838 Step: 14000 of 24172 Loss: 0.039557\n",
      "2019-12-13 01:28:22.998283 Step: 15000 of 24172 Loss: 0.043187\n",
      "2019-12-13 01:30:57.849044 Step: 16000 of 24172 Loss: 0.043270\n",
      "2019-12-13 01:33:32.732001 Step: 17000 of 24172 Loss: 0.044664\n",
      "2019-12-13 01:36:07.063874 Step: 18000 of 24172 Loss: 0.046851\n",
      "2019-12-13 01:38:41.704602 Step: 19000 of 24172 Loss: 0.052012\n",
      "2019-12-13 01:41:17.310151 Step: 20000 of 24172 Loss: 0.043148\n",
      "2019-12-13 01:43:53.151076 Step: 21000 of 24172 Loss: 0.079076\n",
      "2019-12-13 01:46:28.887235 Step: 22000 of 24172 Loss: 0.046330\n",
      "2019-12-13 01:49:04.956391 Step: 23000 of 24172 Loss: 0.038722\n",
      "2019-12-13 01:51:41.109333 Step: 24000 of 24172 Loss: 0.049903\n",
      "Training Loss: 0.046305 for epoch 12\n",
      "Epoch:  12\n",
      "processed 917637 tokens with 33613 phrases; found: 33939 phrases; correct: 32610.\n",
      "accuracy:  97.54%; (non-O)\n",
      "accuracy:  99.74%; precision:  96.08%; recall:  97.02%; FB1:  96.55%\n",
      "              age: precision:  76.81%; recall:  82.36%; FB1:  79.49%  772\n",
      "             date: precision:  98.02%; recall:  98.85%; FB1:  98.43%  22313\n",
      "    hospital_name: precision:  83.81%; recall:  84.81%; FB1:  84.31%  766\n",
      "               id: precision:  98.06%; recall:  97.60%; FB1:  97.83%  2116\n",
      "         location: precision:  93.52%; recall:  93.52%; FB1:  93.52%  1265\n",
      "         org_name: precision:  20.00%; recall:   4.55%; FB1:   7.41%  5\n",
      "      person_name: precision:  93.70%; recall:  94.97%; FB1:  94.33%  6192\n",
      "          room_no: precision:  87.16%; recall:  96.51%; FB1:  91.60%  444\n",
      "     telephone_no: precision:  82.54%; recall:  70.27%; FB1:  75.91%  63\n",
      "          web_url: precision: 100.00%; recall:  25.00%; FB1:  40.00%  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  81%|████████▏ | 13/16 [14:59:35<3:27:37, 4152.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-13 01:58:56.028759 Step: 1 of 24172 Loss: 4.909222\n",
      "2019-12-13 02:01:33.699615 Step: 1000 of 24172 Loss: 0.040588\n",
      "2019-12-13 02:04:05.840964 Step: 2000 of 24172 Loss: 0.035050\n",
      "2019-12-13 02:06:37.251443 Step: 3000 of 24172 Loss: 0.037176\n",
      "2019-12-13 02:09:11.155254 Step: 4000 of 24172 Loss: 0.039330\n",
      "2019-12-13 02:11:47.504091 Step: 5000 of 24172 Loss: 0.037835\n",
      "2019-12-13 02:14:23.147172 Step: 6000 of 24172 Loss: 0.039941\n",
      "2019-12-13 02:16:59.911507 Step: 7000 of 24172 Loss: 0.040710\n",
      "2019-12-13 02:19:34.593193 Step: 8000 of 24172 Loss: 0.041862\n",
      "2019-12-13 02:22:11.698303 Step: 9000 of 24172 Loss: 0.039882\n",
      "2019-12-13 02:24:46.385071 Step: 10000 of 24172 Loss: 0.033654\n",
      "2019-12-13 02:27:22.209620 Step: 11000 of 24172 Loss: 0.047116\n",
      "2019-12-13 02:29:58.841680 Step: 12000 of 24172 Loss: 0.040231\n",
      "2019-12-13 02:32:33.369258 Step: 13000 of 24172 Loss: 0.038147\n",
      "2019-12-13 02:35:06.023981 Step: 14000 of 24172 Loss: 0.046649\n",
      "2019-12-13 02:37:42.987176 Step: 15000 of 24172 Loss: 0.039953\n",
      "2019-12-13 02:40:14.755756 Step: 16000 of 24172 Loss: 0.038693\n",
      "2019-12-13 02:42:52.068777 Step: 17000 of 24172 Loss: 0.050270\n",
      "2019-12-13 02:45:27.263935 Step: 18000 of 24172 Loss: 0.034637\n",
      "2019-12-13 02:47:59.617013 Step: 19000 of 24172 Loss: 0.057472\n",
      "2019-12-13 02:50:37.442544 Step: 20000 of 24172 Loss: 0.036824\n",
      "2019-12-13 02:53:17.007917 Step: 21000 of 24172 Loss: 0.038042\n",
      "2019-12-13 02:55:54.084150 Step: 22000 of 24172 Loss: 0.030193\n",
      "2019-12-13 02:58:27.839641 Step: 23000 of 24172 Loss: 0.036930\n",
      "2019-12-13 03:00:59.008905 Step: 24000 of 24172 Loss: 0.042438\n",
      "Training Loss: 0.039856 for epoch 13\n",
      "Epoch:  13\n",
      "processed 917637 tokens with 33613 phrases; found: 33881 phrases; correct: 32563.\n",
      "accuracy:  97.28%; (non-O)\n",
      "accuracy:  99.73%; precision:  96.11%; recall:  96.88%; FB1:  96.49%\n",
      "              age: precision:  78.93%; recall:  77.50%; FB1:  78.21%  707\n",
      "             date: precision:  97.92%; recall:  98.82%; FB1:  98.37%  22330\n",
      "    hospital_name: precision:  82.84%; recall:  84.81%; FB1:  83.81%  775\n",
      "               id: precision:  98.48%; recall:  97.37%; FB1:  97.92%  2102\n",
      "         location: precision:  93.09%; recall:  93.68%; FB1:  93.38%  1273\n",
      "         org_name: precision:   7.69%; recall:   4.55%; FB1:   5.71%  13\n",
      "      person_name: precision:  94.06%; recall:  94.93%; FB1:  94.49%  6165\n",
      "          room_no: precision:  88.58%; recall:  94.76%; FB1:  91.57%  429\n",
      "     telephone_no: precision:  81.25%; recall:  70.27%; FB1:  75.36%  64\n",
      "          web_url: precision:  43.48%; recall:  83.33%; FB1:  57.14%  23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  88%|████████▊ | 14/16 [16:08:54<2:18:28, 4154.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-13 03:08:14.084269 Step: 1 of 24172 Loss: 4.675876\n",
      "2019-12-13 03:10:50.676050 Step: 1000 of 24172 Loss: 0.035532\n",
      "2019-12-13 03:13:27.747039 Step: 2000 of 24172 Loss: 0.032839\n",
      "2019-12-13 03:16:04.780980 Step: 3000 of 24172 Loss: 0.036365\n",
      "2019-12-13 03:18:36.606264 Step: 4000 of 24172 Loss: 0.032460\n",
      "2019-12-13 03:21:12.468464 Step: 5000 of 24172 Loss: 0.054151\n",
      "2019-12-13 03:23:45.837940 Step: 6000 of 24172 Loss: 0.044133\n",
      "2019-12-13 03:26:24.082840 Step: 7000 of 24172 Loss: 0.029583\n",
      "2019-12-13 03:28:58.593692 Step: 8000 of 24172 Loss: 0.043092\n",
      "2019-12-13 03:31:35.655994 Step: 9000 of 24172 Loss: 0.035773\n",
      "2019-12-13 03:34:09.660020 Step: 10000 of 24172 Loss: 0.029915\n",
      "2019-12-13 03:36:45.626449 Step: 11000 of 24172 Loss: 0.035540\n",
      "2019-12-13 03:39:21.840079 Step: 12000 of 24172 Loss: 0.035739\n",
      "2019-12-13 03:41:54.458087 Step: 13000 of 24172 Loss: 0.037062\n",
      "2019-12-13 03:44:25.695762 Step: 14000 of 24172 Loss: 0.033560\n",
      "2019-12-13 03:47:04.148614 Step: 15000 of 24172 Loss: 0.035115\n",
      "2019-12-13 03:49:37.409277 Step: 16000 of 24172 Loss: 0.030709\n",
      "2019-12-13 03:52:13.887413 Step: 17000 of 24172 Loss: 0.035463\n",
      "2019-12-13 03:54:48.428143 Step: 18000 of 24172 Loss: 0.029515\n",
      "2019-12-13 03:57:22.435998 Step: 19000 of 24172 Loss: 0.030369\n",
      "2019-12-13 03:59:54.681301 Step: 20000 of 24172 Loss: 0.033604\n",
      "2019-12-13 04:02:30.528524 Step: 21000 of 24172 Loss: 0.035089\n",
      "2019-12-13 04:05:07.643614 Step: 22000 of 24172 Loss: 0.034538\n",
      "2019-12-13 04:07:44.721459 Step: 23000 of 24172 Loss: 0.032262\n",
      "2019-12-13 04:10:16.211866 Step: 24000 of 24172 Loss: 0.031641\n",
      "Training Loss: 0.034992 for epoch 14\n",
      "Epoch:  14\n",
      "processed 917637 tokens with 33613 phrases; found: 33805 phrases; correct: 32584.\n",
      "accuracy:  97.31%; (non-O)\n",
      "accuracy:  99.74%; precision:  96.39%; recall:  96.94%; FB1:  96.66%\n",
      "              age: precision:  78.56%; recall:  78.89%; FB1:  78.72%  723\n",
      "             date: precision:  98.32%; recall:  98.90%; FB1:  98.61%  22259\n",
      "    hospital_name: precision:  84.83%; recall:  83.49%; FB1:  84.15%  745\n",
      "               id: precision:  98.20%; recall:  97.60%; FB1:  97.90%  2113\n",
      "         location: precision:  92.88%; recall:  93.83%; FB1:  93.35%  1278\n",
      "         org_name: precision:  20.00%; recall:   9.09%; FB1:  12.50%  10\n",
      "      person_name: precision:  94.01%; recall:  94.84%; FB1:  94.43%  6163\n",
      "          room_no: precision:  88.99%; recall:  94.76%; FB1:  91.79%  427\n",
      "     telephone_no: precision:  79.69%; recall:  68.92%; FB1:  73.91%  64\n",
      "          web_url: precision:  47.83%; recall:  91.67%; FB1:  62.86%  23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  94%|█████████▍| 15/16 [17:18:10<1:09:14, 4154.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-13 04:17:30.067295 Step: 1 of 24172 Loss: 6.447200\n",
      "2019-12-13 04:20:02.270006 Step: 1000 of 24172 Loss: 0.035373\n",
      "2019-12-13 04:22:38.827936 Step: 2000 of 24172 Loss: 0.029234\n",
      "2019-12-13 04:25:14.751667 Step: 3000 of 24172 Loss: 0.024756\n",
      "2019-12-13 04:27:53.674561 Step: 4000 of 24172 Loss: 0.029656\n",
      "2019-12-13 04:30:26.992796 Step: 5000 of 24172 Loss: 0.031455\n",
      "2019-12-13 04:33:03.436020 Step: 6000 of 24172 Loss: 0.033521\n",
      "2019-12-13 04:35:39.356252 Step: 7000 of 24172 Loss: 0.052066\n",
      "2019-12-13 04:38:18.454087 Step: 8000 of 24172 Loss: 0.031927\n",
      "2019-12-13 04:40:55.807147 Step: 9000 of 24172 Loss: 0.026125\n",
      "2019-12-13 04:43:31.520520 Step: 10000 of 24172 Loss: 0.023471\n",
      "2019-12-13 04:46:03.255517 Step: 11000 of 24172 Loss: 0.033073\n",
      "2019-12-13 04:48:33.384877 Step: 12000 of 24172 Loss: 0.028970\n",
      "2019-12-13 04:51:09.746671 Step: 13000 of 24172 Loss: 0.023806\n",
      "2019-12-13 04:53:44.457899 Step: 14000 of 24172 Loss: 0.030822\n",
      "2019-12-13 04:56:17.531598 Step: 15000 of 24172 Loss: 0.030949\n",
      "2019-12-13 04:58:53.266448 Step: 16000 of 24172 Loss: 0.036225\n",
      "2019-12-13 05:01:30.295382 Step: 17000 of 24172 Loss: 0.035401\n",
      "2019-12-13 05:04:07.945208 Step: 18000 of 24172 Loss: 0.027030\n",
      "2019-12-13 05:06:47.440640 Step: 19000 of 24172 Loss: 0.035071\n",
      "2019-12-13 05:09:20.423036 Step: 20000 of 24172 Loss: 0.026989\n",
      "2019-12-13 05:11:52.283798 Step: 21000 of 24172 Loss: 0.030827\n",
      "2019-12-13 05:14:22.409975 Step: 22000 of 24172 Loss: 0.029367\n",
      "2019-12-13 05:16:56.047939 Step: 23000 of 24172 Loss: 0.032960\n",
      "2019-12-13 05:19:26.204033 Step: 24000 of 24172 Loss: 0.029761\n",
      "Training Loss: 0.030883 for epoch 15\n",
      "Epoch:  15\n",
      "processed 917637 tokens with 33613 phrases; found: 33812 phrases; correct: 32576.\n",
      "accuracy:  97.36%; (non-O)\n",
      "accuracy:  99.74%; precision:  96.34%; recall:  96.91%; FB1:  96.63%\n",
      "              age: precision:  78.58%; recall:  76.94%; FB1:  77.75%  705\n",
      "             date: precision:  98.25%; recall:  98.93%; FB1:  98.59%  22281\n",
      "    hospital_name: precision:  84.62%; recall:  84.28%; FB1:  84.45%  754\n",
      "               id: precision:  98.16%; recall:  97.60%; FB1:  97.88%  2114\n",
      "         location: precision:  93.90%; recall:  93.68%; FB1:  93.79%  1262\n",
      "         org_name: precision:   9.09%; recall:   4.55%; FB1:   6.06%  11\n",
      "      person_name: precision:  93.84%; recall:  94.81%; FB1:  94.32%  6172\n",
      "          room_no: precision:  88.99%; recall:  94.76%; FB1:  91.79%  427\n",
      "     telephone_no: precision:  77.78%; recall:  66.22%; FB1:  71.53%  63\n",
      "          web_url: precision:  47.83%; recall:  91.67%; FB1:  62.86%  23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 16/16 [18:27:20<00:00, 4153.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time:  66440.186170461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for epoch in (train_iterator):\n",
    "    epoch_iterator = tqdm(train_iter, desc=\"Iteration\", disable=-1)\n",
    "    tr_loss = 0.0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        s = timeit.default_timer()\n",
    "        token_ids, attn_mask, orig_2_map, labels, original_tok, _= batch\n",
    "        #print(labels)\n",
    "        inputs = {'input_ids' : token_ids.to(device),\n",
    "                 'attn_masks' : attn_mask.to(device),\n",
    "                 'labels' : labels.to(device)\n",
    "                 }  \n",
    "        loss= model(**inputs) \n",
    "        loss.backward()\n",
    "        tmp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % 1 == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "        if step == 0:\n",
    "            print('%s Step: %d of %d Loss: %f' %(str(datetime.datetime.now()), (step+1), len(epoch_iterator), tmp_loss))\n",
    "        if (step+1) % 1000 == 0:\n",
    "            print('%s Step: %d of %d Loss: %f' %(str(datetime.datetime.now()), (step+1), len(epoch_iterator), tmp_loss/1000))\n",
    "            tmp_loss = 0.0\n",
    "  \n",
    "    print(\"Training Loss: %f for epoch %d\" %(tr_loss/len(train_iter), epoch))\n",
    "    training_loss.append(tr_loss/len(train_iter))\n",
    "    #'''\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    writer = open(APR_DIR + 'prediction_'+str(epoch)+'.csv', 'w')\n",
    "    for i, batch in enumerate(eval_iter):\n",
    "        fname = dev_file_names[i][0]\n",
    "        token_ids, attn_mask, orig_2_map, labels, original_token, filenames = batch\n",
    "        #attn_mask.dt\n",
    "        inputs = {'input_ids': token_ids.to(device),\n",
    "                  'attn_masks' : attn_mask.to(device)\n",
    "                 }  \n",
    "        \n",
    "        dev_inputs = {'input_ids' : token_ids.to(device),\n",
    "                     'attn_masks' : attn_mask.to(device),\n",
    "                     'labels' : labels.to(device)\n",
    "                     } \n",
    "        with torch.torch.no_grad():\n",
    "            tag_seqs = model(**inputs)\n",
    "            tmp_eval_loss = model(**dev_inputs)\n",
    "        val_loss += tmp_eval_loss.item()\n",
    "        #print(labels.numpy())\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        for i, o2m in enumerate(orig_2_map):\n",
    "            for j, orig_tok_idx in enumerate(o2m):\n",
    "                writer.write(original_token[i][j] + '\\t')\n",
    "                writer.write(filenames[i][j] + '\\t')\n",
    "                #writer.write(str(scores[i][j]) + '\\t')\n",
    "                writer.write('NA\\t')\n",
    "                writer.write(unique_labels[y_true[i][orig_tok_idx]] + '\\t')\n",
    "                pred_tag = unique_labels[tag_seqs[i][orig_tok_idx]]\n",
    "                if pred_tag == 'X':\n",
    "                    pred_tag = 'O'\n",
    "                writer.write(pred_tag + '\\n')\n",
    "                Y_true.append(unique_labels[y_true[i][orig_tok_idx]])\n",
    "                Y_pred.append(pred_tag)\n",
    "            writer.write('\\n')\n",
    "            \n",
    "    validation_loss.append(val_loss/len(eval_iter))\n",
    "    writer.flush()\n",
    "    print('Epoch: ', epoch)\n",
    "    #print(classification_report(y_pred=Y_pred, y_true=Y_true))\n",
    "    command = \"python conlleval.py < \" + APR_DIR + \"prediction_\"+str(epoch)+\".csv\"\n",
    "    process = subprocess.Popen(command,stdout=subprocess.PIPE, shell=True)\n",
    "    result = process.communicate()[0].decode(\"utf-8\")\n",
    "    print(result)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': tr_loss/len(train_iter),\n",
    "    }, APR_DIR + 'model_' + str(epoch) + '.pt')\n",
    "\n",
    "total_time = timeit.default_timer() - start_time\n",
    "print('Total training time: ',   total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "plt.plot(range(1,17), training_loss, label='Training Loss')\n",
    "plt.plot(range(1,17), validation_loss, label='Testing Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title(\"Training Loss Vs Testing Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(APR_DIR + 'Loss.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
